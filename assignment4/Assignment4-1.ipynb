{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT405 Introduction to Data Science and AI \n",
    "## 2022-2023, Reading Period 2\n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "There will be an overall grade for this assignment. To get a pass grade (grade 5), you need to pass items 1-3 below. To receive higher grades, finish items 4 and 5 as well. \n",
    "\n",
    "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use \n",
    "7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Wa37fBwRF-xe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '20021010_easy_ham.tar.bz2'\n",
      "tar: Error opening archive: Failed to open '20021010_hard_ham.tar.bz2'\n",
      "tar: Error opening archive: Failed to open '20021010_spam.tar.bz2'\n"
     ]
    }
   ],
   "source": [
    "#Download and extract data\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "# !tar -xjf 20021010_easy_ham.tar.bz2\n",
    "# !tar -xjf 20021010_hard_ham.tar.bz2\n",
    "# !tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "A53Gw00fBLG2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "### 1. Preprocessing: \n",
    "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher-grade part), you will be asked to filter out the headers and footers. \n",
    "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "\n",
    "# We extracted the directories beforehand and not in the file\n",
    "\n",
    "# Get all file names in the different directories\n",
    "easy_ham_files = listdir(\"./easy_ham\")\n",
    "hard_ham_files = listdir(\"./hard_ham\")\n",
    "spam_files = listdir(\"./spam\")\n",
    "\n",
    "easy_ham = []\n",
    "hard_ham = []\n",
    "spam = []\n",
    "\n",
    "# Load in the content of all files into lists\n",
    "for file_name in easy_ham_files:\n",
    "    fd = open(f\"./easy_ham/{file_name}\", \"r\")\n",
    "    easy_ham.append(fd.read())\n",
    "    \n",
    "for file_name in hard_ham_files:\n",
    "    fd = open(f\"./hard_ham/{file_name}\", \"r\")\n",
    "    hard_ham.append(fd.read())\n",
    "\n",
    "for file_name in spam_files:\n",
    "    fd = open(f\"./spam/{file_name}\", \"r\", encoding=\"unicode_escape\")\n",
    "    spam.append(fd.read())\n",
    "    \n",
    "vect = CountVectorizer()\n",
    "    \n",
    "# Combine the emails, create y array where 1 is ham and 0 is spam and then split the data into sets\n",
    "emails = vect.fit_transform(easy_ham + hard_ham + spam)\n",
    "Y = ([1] * len(easy_ham)) + ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails, Y, test_size=0.2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "### 2. Write a Python program that: \n",
    "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n",
    "- Multinomial Naive Bayes  \n",
    "- Bernoulli Naive Bayes. \n",
    "\n",
    "Please inspect the documentation to ensure input to the classifiers is appropriate. Discuss the differences between these two classifiers. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "\n",
    "# instantiate the models\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# fit the data to the models\n",
    "bnb.fit(X_train, Y_train)\n",
    "mnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1bPDCvQxen"
   },
   "source": [
    "##### We have also submitted a pdf report with the same content but in a more structured way.\n",
    "\n",
    "According to the documentation one explicit difference, except the calculation method, is that BernoulliNB penalizes a non-occurance of a feature.\n",
    "This means that, for example, if the word \"investment\" often occurs in spam emails and it is not seen when it tries to classify an email it is less likely that it is a spam email.\n",
    "MultinomialNB does not do this but instead ignores the lack-off a word it has classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.Run your program on \n",
    "-\tSpam versus easy-ham \n",
    "-\tSpam versus hard-ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gool_zb8Qzzy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9878971255673222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        94\n",
      "           1       0.99      0.99      0.99       567\n",
      "\n",
      "    accuracy                           0.99       661\n",
      "   macro avg       0.98      0.98      0.98       661\n",
      "weighted avg       0.99      0.99      0.99       661\n",
      "\n",
      "Berniulli: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8956127080181543"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.29      0.44        94\n",
      "           1       0.89      1.00      0.94       567\n",
      "\n",
      "    accuracy                           0.90       661\n",
      "   macro avg       0.91      0.64      0.69       661\n",
      "weighted avg       0.90      0.90      0.87       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code to report results here\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# predict using the test set and print the results\n",
    "Y_pred_mnb = mnb.predict(X_test)\n",
    "print(\"Multinomial: \")\n",
    "display(accuracy_score(Y_test, Y_pred_mnb))\n",
    "print(classification_report(Y_test, Y_pred_mnb))\n",
    "\n",
    "\n",
    "Y_pred_bnb = bnb.predict(X_test)\n",
    "print(\"Berniulli: \")\n",
    "display(accuracy_score(Y_test, Y_pred_bnb))\n",
    "print(classification_report(Y_test, Y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkfQWBB4UhYd"
   },
   "source": [
    "### 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n",
    "\n",
    "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. \n",
    "\n",
    "**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report your results.\n",
    "\n",
    "You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qt7ELzEqUfas",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9939485627836612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98        96\n",
      "           1       0.99      1.00      1.00       565\n",
      "\n",
      "    accuracy                           0.99       661\n",
      "   macro avg       0.99      0.98      0.99       661\n",
      "weighted avg       0.99      0.99      0.99       661\n",
      "\n",
      "Berniulli: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8835098335854765"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.26      0.39        96\n",
      "           1       0.89      0.99      0.94       565\n",
      "\n",
      "    accuracy                           0.88       661\n",
      "   macro avg       0.85      0.62      0.66       661\n",
      "weighted avg       0.88      0.88      0.86       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove uninformative words using count vectorizer\n",
    "vect2 = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "\n",
    "# Do the same as in part 2/3\n",
    "emails = vect2.fit_transform(easy_ham + hard_ham + spam)\n",
    "Y = ([1] * len(easy_ham)) + ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails, Y, test_size=0.2)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "\n",
    "bnb.fit(X_train, Y_train)\n",
    "mnb.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_mnb = mnb.predict(X_test)\n",
    "print(\"Multinomial: \")\n",
    "display(accuracy_score(Y_test, Y_pred_mnb))\n",
    "print(classification_report(Y_test, Y_pred_mnb))\n",
    "\n",
    "\n",
    "Y_pred_bnb = bnb.predict(X_test)\n",
    "print(\"Berniulli: \")\n",
    "display(accuracy_score(Y_test, Y_pred_bnb))\n",
    "print(classification_report(Y_test, Y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common word will show up in both ham and spam and will therefore not be indicative of either class.\n",
    "Even if we filter out the stop_words there will be some stragglers left that we want to remove.\n",
    "There is also a downside to this. \n",
    "Some words might be very common but only occur in one of the classes, by removing them we will lose that data and might get a worse fit.\n",
    "\n",
    "Words that only occur very few times does not provide much indication of if it is a ham or spam word since there are too few occurrences.\n",
    "You might get an email regarding a very specific topic that is ham, and that email is the only occurrence of that word.\n",
    "Then you get a spam email that has that same word, then you don't want to use that single occurrence to classify with since it it very indicative of ham where the rest of the email might not have been.\n",
    "\n",
    "We have used both stop_words and strip_accents when filtering with CountVectorizer.\n",
    "We decided to use the built in functions since they provide a good base set to use while it is very hard to create our own.\n",
    "Stop_words contains a list of common English words that do not provide meaning but are instead a function of grammar.\n",
    "This list would take a lot of manual labour to create and using the ready-made one from CountVectorizer provide an easy to use solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyVfOZFU4F_"
   },
   "source": [
    "### 5. Eeking out further performance\n",
    "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \n",
    "-\tDoes the result improve from 3 and 4? \n",
    "- The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n",
    "- What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages? \n",
    "\n",
    "Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n",
    "- What does this parameter mean?\n",
    "- How does this alter the predictions? Discuss why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "s_nyGug9U4f3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863842662632375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95        84\n",
      "           1       0.99      0.99      0.99       577\n",
      "\n",
      "    accuracy                           0.99       661\n",
      "   macro avg       0.97      0.97      0.97       661\n",
      "weighted avg       0.99      0.99      0.99       661\n",
      "\n",
      "Berniulli: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.903177004538578"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.35      0.48        84\n",
      "           1       0.91      0.98      0.95       577\n",
      "\n",
      "    accuracy                           0.90       661\n",
      "   macro avg       0.84      0.66      0.71       661\n",
      "weighted avg       0.89      0.90      0.89       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reset the data in the arrays\n",
    "easy_ham = []\n",
    "hard_ham = []\n",
    "spam = []\n",
    "\n",
    "\n",
    "# Remove some of the header, specifically from the word \"From:\" which occurs far down in the header\n",
    "for file_name in easy_ham_files:\n",
    "    fd = open(f\"./easy_ham/{file_name}\", \"r\")\n",
    "    tmp = fd.read().split(\"\\nFrom: \")\n",
    "    easy_ham.append(tmp[len(tmp) - 1])\n",
    "    \n",
    "for file_name in hard_ham_files:\n",
    "    fd = open(f\"./hard_ham/{file_name}\", \"r\")\n",
    "    tmp = fd.read().split(\"\\nFrom: \")\n",
    "    hard_ham.append(tmp[len(tmp) - 1])\n",
    "\n",
    "for file_name in spam_files:\n",
    "    fd = open(f\"./spam/{file_name}\", \"r\", encoding=\"unicode_escape\")\n",
    "    tmp = fd.read().split(\"\\nFrom: \")\n",
    "    spam.append(tmp[len(tmp) - 1])\n",
    "      \n",
    "# Do the same as in part 2/3/4  \n",
    "vect3 = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "emails = vect3.fit_transform(easy_ham + hard_ham + spam)\n",
    "\n",
    "Y = ([1] * len(easy_ham)) + ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails, Y, test_size=0.2)\n",
    "\n",
    "# remove comment and extra ) to run with fit_prior=False\n",
    "bnb = BernoulliNB()#fit_prior=False)\n",
    "mnb = MultinomialNB()#fit_prior=False)\n",
    "\n",
    "bnb.fit(X_train, Y_train)\n",
    "mnb.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_mnb = mnb.predict(X_test)\n",
    "print(\"Multinomial: \")\n",
    "display(accuracy_score(Y_test, Y_pred_mnb))\n",
    "print(classification_report(Y_test, Y_pred_mnb))\n",
    "\n",
    "\n",
    "Y_pred_bnb = bnb.predict(X_test)\n",
    "print(\"Berniulli: \")\n",
    "display(accuracy_score(Y_test, Y_pred_bnb))\n",
    "print(classification_report(Y_test, Y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND6FKoexVAhW"
   },
   "source": [
    "Multinomial increased a bit from part 3 but not much from part 4. \n",
    "We can see that the accuracy score metric stayed almost the same but that the precision number went up with 0,07.\n",
    "Filtering the data from step 3 to 4 seems to have a greater impact on accuracy than filtering out the headers which makes sense considering that the headers should be the same for most emails leaving the words in the headers to not have much impact on the classification.\n",
    "As for Bernoulli we can see improvements form part 3 and part 4, accuracy increased as well as precision.\n",
    "\n",
    "Since we are splitting the data in random we might have some spam messages that are very unique in the testing or in the training sets.\n",
    "The results might be skewed if we by chance get a lot of the same category of spam mails, for example phishing, in the test set but not in the training set.\n",
    "Then it will be hard for the model to accurately predict that an email is spam on the data it has been fitted with.\n",
    "\n",
    "We came up with two ways to try and remedy this.\n",
    "Firstly is to manually categorize the data beforehand and split the training and test sets evenly over the categories.\n",
    "For example if you have the spam categories of Phishing, malware, unwanted ads and a general spam one, you could then split those in accordance to your testing/training ratios to make sure that you train on all categories evenly.\n",
    "This does however have the drawback of a lot of manual labor and might not be suitable for large data sets.\n",
    "\n",
    "Another way to try and do it pragmatically is to count the words before you do the split and try to make sure that both sets contains the largest vocabularies possible.\n",
    "This would try to accomplish the same as the other method on the assumption that, for example, phishing emails generally contains the same vocabulary.\n",
    "\n",
    "With fit prior on the model calculates the probability of a word being in either ham or spam.\n",
    "It uses these weights to try and find if an email is either a spam or a ham email.\n",
    "When fit_prior is turned of it just uses the occurrences of words the email when it tries to classify it with the probability being an uniform distribution.\n",
    "This means that if a word is both in spam and ham it will basically be a non factor since it can not determine if it is more likely that it belongs to either.\n",
    "\n",
    "This lowers the score for both classifiers which makes a lot of sense considering it has less information to use.\n",
    "The weighted prior gives a lot of data if a word is more or less common in either of the cases and not being able to use that data reduces the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bI3z_spVacz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "72aacd22952597d5f9d461d9ef42ba91a1e3c323af4fa9a8b3e0327b6031588d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
