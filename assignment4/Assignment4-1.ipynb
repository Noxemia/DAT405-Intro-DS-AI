{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT405 Introduction to Data Science and AI \n",
    "## 2022-2023, Reading Period 2\n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "There will be an overall grade for this assignment. To get a pass grade (grade 5), you need to pass items 1-3 below. To receive higher grades, finish items 4 and 5 as well. \n",
    "\n",
    "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use \n",
    "7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wa37fBwRF-xe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '20021010_easy_ham.tar.bz2'\n",
      "tar: Error opening archive: Failed to open '20021010_hard_ham.tar.bz2'\n",
      "tar: Error opening archive: Failed to open '20021010_spam.tar.bz2'\n"
     ]
    }
   ],
   "source": [
    "#Download and extract data\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "!tar -xjf 20021010_easy_ham.tar.bz2\n",
    "!tar -xjf 20021010_hard_ham.tar.bz2\n",
    "!tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A53Gw00fBLG2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "###1. Preprocessing: \n",
    "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher-grade part), you will be asked to filter out the headers and footers. \n",
    "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "\n",
    "easy_ham_files = listdir(\"./easy_ham\")\n",
    "hard_ham_files = listdir(\"./hard_ham\")\n",
    "spam_files = listdir(\"./spam\")\n",
    "\n",
    "easy_ham = []\n",
    "hard_ham = []\n",
    "spam = []\n",
    "\n",
    "for file_name in easy_ham_files:\n",
    "    fd = open(f\"./easy_ham/{file_name}\", \"r\")\n",
    "    easy_ham.append(fd.read())\n",
    "    \n",
    "for file_name in hard_ham_files:\n",
    "    fd = open(f\"./hard_ham/{file_name}\", \"r\")\n",
    "    hard_ham.append(fd.read())\n",
    "\n",
    "for file_name in spam_files:\n",
    "    fd = open(f\"./spam/{file_name}\", \"r\", encoding=\"unicode_escape\")\n",
    "    spam.append(fd.read())\n",
    "    \n",
    "vect = CountVectorizer()\n",
    "size = 0.2\n",
    "    \n",
    "# Combined emails\n",
    "emails = vect.fit_transform(easy_ham + hard_ham + spam)\n",
    "Y = ([1] * len(easy_ham)) + ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails, Y, test_size=size)\n",
    "    \n",
    "# Easy emails\n",
    "#emails_easy = vect.fit_transform(easy_ham + spam)\n",
    "#Y_easy = ([1] * len(easy_ham)) + ([0] * len(spam))\n",
    "#X_train_easy, X_test_easy, Y_train_easy, Y_test_easy = train_test_split(emails_easy, Y_easy, test_size=size)\n",
    "    \n",
    "# Hard emails\n",
    "#emails_hard = vect.fit_transform(hard_ham + spam)\n",
    "#Y_hard = ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "#X_train_hard, X_test_hard, Y_train_hard, Y_test_hard = train_test_split(emails_hard, Y_hard, test_size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "###2. Write a Python program that: \n",
    "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n",
    "- Multinomial Naive Bayes  \n",
    "- Bernoulli Naive Bayes. \n",
    "\n",
    "Please inspect the documentation to ensure input to the classifiers is appropriate. Discuss the differences between these two classifiers. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "\n",
    "bnb.fit(X_train, Y_train)\n",
    "mnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1bPDCvQxen"
   },
   "source": [
    "Your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.Run your program on \n",
    "-\tSpam versus easy-ham \n",
    "-\tSpam versus hard-ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gool_zb8Qzzy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9878971255673222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        85\n",
      "           1       0.99      0.99      0.99       576\n",
      "\n",
      "    accuracy                           0.99       661\n",
      "   macro avg       0.98      0.97      0.97       661\n",
      "weighted avg       0.99      0.99      0.99       661\n",
      "\n",
      "Berniulli: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9122541603630863"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.35      0.51        85\n",
      "           1       0.91      0.99      0.95       576\n",
      "\n",
      "    accuracy                           0.91       661\n",
      "   macro avg       0.91      0.67      0.73       661\n",
      "weighted avg       0.91      0.91      0.89       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code to report results here\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "Y_pred_mnb = mnb.predict(X_test)\n",
    "print(\"Multinomial: \")\n",
    "display(accuracy_score(Y_test, Y_pred_mnb))\n",
    "print(classification_report(Y_test, Y_pred_mnb))\n",
    "\n",
    "\n",
    "Y_pred_bnb = bnb.predict(X_test)\n",
    "print(\"Berniulli: \")\n",
    "display(accuracy_score(Y_test, Y_pred_bnb))\n",
    "print(classification_report(Y_test, Y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkfQWBB4UhYd"
   },
   "source": [
    "### 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n",
    "\n",
    "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. \n",
    "\n",
    "**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report your results.\n",
    "\n",
    "You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qt7ELzEqUfas",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63284</th>\n",
       "      <td>v5r</td>\n",
       "      <td>100999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83764</th>\n",
       "      <td>v5p79385</td>\n",
       "      <td>100998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98037</th>\n",
       "      <td>v5khacaqaw0lcqrbb2okmes</td>\n",
       "      <td>100997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63518</th>\n",
       "      <td>v5g</td>\n",
       "      <td>100996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107091</th>\n",
       "      <td>v5fquxjfora7vlj0llyr</td>\n",
       "      <td>100995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78313</th>\n",
       "      <td>00002f296eae</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77899</th>\n",
       "      <td>00002e3e</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76683</th>\n",
       "      <td>00002d86</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87269</th>\n",
       "      <td>00002d511d6b</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103158</th>\n",
       "      <td>00002b4f</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100899 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Word   Count\n",
       "63284                       v5r  100999\n",
       "83764                  v5p79385  100998\n",
       "98037   v5khacaqaw0lcqrbb2okmes  100997\n",
       "63518                       v5g  100996\n",
       "107091     v5fquxjfora7vlj0llyr  100995\n",
       "...                         ...     ...\n",
       "78313              00002f296eae     105\n",
       "77899                  00002e3e     104\n",
       "76683                  00002d86     103\n",
       "87269              00002d511d6b     102\n",
       "103158                 00002b4f     101\n",
       "\n",
       "[100899 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9818456883509834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93        90\n",
      "           1       0.98      1.00      0.99       571\n",
      "\n",
      "    accuracy                           0.98       661\n",
      "   macro avg       0.98      0.94      0.96       661\n",
      "weighted avg       0.98      0.98      0.98       661\n",
      "\n",
      "Berniulli: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8940998487140696"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.27      0.41        90\n",
      "           1       0.90      0.99      0.94       571\n",
      "\n",
      "    accuracy                           0.89       661\n",
      "   macro avg       0.88      0.63      0.67       661\n",
      "weighted avg       0.89      0.89      0.87       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#display(vect.vocabulary_)\n",
    "\n",
    "words = pd.DataFrame(vect.vocabulary_.items(), columns=['Word', 'Count'])\n",
    "words = words[words['Count'] < 101000]\n",
    "words = words[words['Count'] > 100]\n",
    "words = words.sort_values(by=[\"Count\"], ascending=False)\n",
    "display(words)\n",
    "voc = words['Word'].tolist()\n",
    "\n",
    "vect2 = CountVectorizer(strip_accents='unicode', stop_words='english', vocabulary=voc)# max_df=0.000303, min_df=0.0001)\n",
    "emails = vect2.fit_transform(easy_ham + hard_ham + spam)\n",
    "Y = ([1] * len(easy_ham)) + ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails, Y, test_size=size)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "\n",
    "bnb.fit(X_train, Y_train)\n",
    "mnb.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_mnb = mnb.predict(X_test)\n",
    "print(\"Multinomial: \")\n",
    "display(accuracy_score(Y_test, Y_pred_mnb))\n",
    "print(classification_report(Y_test, Y_pred_mnb))\n",
    "\n",
    "\n",
    "Y_pred_bnb = bnb.predict(X_test)\n",
    "print(\"Berniulli: \")\n",
    "display(accuracy_score(Y_test, Y_pred_bnb))\n",
    "print(classification_report(Y_test, Y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyVfOZFU4F_"
   },
   "source": [
    "### 5. Eeking out further performance\n",
    "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \n",
    "-\tDoes the result improve from 3 and 4? \n",
    "- The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n",
    "- What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages? \n",
    "\n",
    "Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n",
    "- What does this parameter mean?\n",
    "- How does this alter the predictions? Discuss why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "s_nyGug9U4f3",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3976\\3048738918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mvect3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unicode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# max_df=0.000303, min_df=0.0001)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0memails\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0measy_ham\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhard_ham\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mspam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memails\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0measy_ham\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhard_ham\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memails\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "#vect3 = CountVectorizer()\n",
    "#v = vect3.fit_transform(easy_ham + hard_ham)\n",
    "#w = pd.DataFrame(vect3.vocabulary_.items(), columns=['Word', 'Count'])\n",
    "#w = w.sort_values(by=[\"Count\"], ascending=False)\n",
    "#display(len(easy_ham + hard_ham))\n",
    "#display(w[w[\"Word\"] == \"from\"])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "easy_ham_files = listdir(\"./easy_ham\")\n",
    "hard_ham_files = listdir(\"./hard_ham\")\n",
    "spam_files = listdir(\"./spam\")\n",
    "\n",
    "easy_ham = []\n",
    "hard_ham = []\n",
    "spam = []\n",
    "\n",
    "for file_name in easy_ham_files:\n",
    "    fd = open(f\"./easy_ham/{file_name}\", \"r\")\n",
    "    tmp = fd.read().split(\"\\nFrom: \")\n",
    "    easy_ham.append(tmp[len(tmp) - 1])\n",
    "    \n",
    "for file_name in hard_ham_files:\n",
    "    fd = open(f\"./hard_ham/{file_name}\", \"r\")\n",
    "    tmp = fd.read().split(\"\\nFrom: \")\n",
    "    easy_ham.append(tmp[len(tmp) - 1])\n",
    "\n",
    "for file_name in spam_files:\n",
    "    fd = open(f\"./spam/{file_name}\", \"r\", encoding=\"unicode_escape\")\n",
    "    tmp = fd.read().split(\"\\nFrom: \")\n",
    "    easy_ham.append(tmp[len(tmp) - 1])\n",
    "\n",
    "                    \n",
    "                    \n",
    "vect3 = CountVectorizer(strip_accents='unicode', stop_words='english', vocabulary=voc)# max_df=0.000303, min_df=0.0001)\n",
    "emails = vect3.fit_transform(easy_ham + hard_ham + spam)\n",
    "\n",
    "Y = ([1] * len(easy_ham)) + ([1] * len(hard_ham)) + ([0] * len(spam))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails, Y, test_size=size)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "bnb.fit(X_train, Y_train)\n",
    "mnb.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_mnb = mnb.predict(X_test)\n",
    "print(\"Multinomial: \")\n",
    "display(accuracy_score(Y_test, Y_pred_mnb))\n",
    "print(classification_report(Y_test, Y_pred_mnb))\n",
    "\n",
    "\n",
    "Y_pred_bnb = bnb.predict(X_test)\n",
    "print(\"Berniulli: \")\n",
    "display(accuracy_score(Y_test, Y_pred_bnb))\n",
    "print(classification_report(Y_test, Y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND6FKoexVAhW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bI3z_spVacz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "72aacd22952597d5f9d461d9ef42ba91a1e3c323af4fa9a8b3e0327b6031588d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
